{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059b4967",
   "metadata": {},
   "source": [
    "#### Predicci칩n de secuencias de palabras\n",
    "#### Emmanuel Santos Rodr칤guez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95bce80",
   "metadata": {},
   "source": [
    "# Introducci칩n\n",
    "Los modelos de lenguaje nos permiten asignar probabilidades a las secuencias de palabras. Mediante estos modelos, podemos predecir cu치l ser치 la siguiente palabra en una secuencia dada o obtener la probabilidad de toda una oraci칩n. Los n-gramas, que son secuencias de n palabras, son el modelo m치s sencillo de este tipo. En particular, usando bigramas podemos aproximar la probabilidad de una palabra dadas todas las palabras previas utilizando solo la probabilidad condicional de la palabra anterior. Para estimar las probabilidades para los bigramas se usa el estimador de m치xima verosimilitud (MLE). Obtenemos el MLE para los par치metros de un modelo de n-gramas obteniendo recuentos de un corpus y normalizando los recuentos para que se encuentren entre 0 y 1.\n",
    "\n",
    "El prop칩sito de este documento es realizar la predicci칩n de secuencias de palabras utilizando un modelo de bigramas con elestimador de m치xima verosimilitud con y sin suavizado para un corpus de textos en espa침ol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871f3ce",
   "metadata": {},
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a39eb2",
   "metadata": {},
   "source": [
    "## Descripci칩n de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d613d",
   "metadata": {},
   "source": [
    "Para el desarrollo de esta soluci칩n se utiliz칩 un corpus de documentos de una conferencia del parlamento europeo. Cada l칤nea es un p치rrafo que considera a nivel de oraci칩n. A continuaci칩n se muestra una oraci칩n ejemplo del corpus:\n",
    "\n",
    ">yo como austr칤aco , pero creo que todos nosotros tenemos a칰n un vivo recuerdo de la cat치strofe que el a침o pasado cost칩 la vida a numerosas personas en el t칰nel del tauern y en el que despu칠s hubo que reconstruir durante largos meses y con un gigantesco gasto financiero lo que fue destruido por el incendio en el t칰nel .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23fc42",
   "metadata": {},
   "source": [
    "## Implementaci칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e954c4e",
   "metadata": {},
   "source": [
    "Para la implementaci칩n de esta soluci칩n se utiliz칩 Python. A continuaci칩n se muestra el c칩digo para los puntos m치s importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c47aa8",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f542924",
   "metadata": {},
   "source": [
    "Antes de construir nuestro modelo de n-gramas, en particular bigramas, debemos realizar un procesamiento para el corpus descrito anteriormente. \n",
    "Este procesamiento consiste en convertir a min칰sculas, eliminar puntuaci칩n y a침adir marcadores de inicio y final para cada oraci칩n. En este caso, usamos \\<s> como marcador de inicio y \\</s> como marcador de final. \n",
    "Al estar conformado por documentos oficiales, se considera que el corpus est치 bien escrito.\n",
    "Es importante recalcar que se considera cada l칤nea como una oraci칩n sin necesidad de encontrar las oraciones por cada p치rrafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    #convertir a minuscula y remover signos de puntuacion\n",
    "    text_lower = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    delimited_text = f\"<s> {text_lower} </s>\"\n",
    "    #obtenemos los tokens\n",
    "    text_tokens = delimited_text.split()\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a68bad",
   "metadata": {},
   "source": [
    "La funci칩n anterior se encarga de convertir el texto a min칰scula, remover los signos de puntuaci칩n y a침adir los marcadores de inicio y fin de oraci칩n as칤 como dividir el texto en tokens.\n",
    "\n",
    "De esta forma, despu칠s de aplicar el procesamiento a la oraci칩n de ejemplo anterior, obtenemos lo siguiente: \n",
    "\n",
    "> \\<s> yo como austr칤aco pero creo que todos nosotros tenemos a칰n un vivo recuerdo de la cat치strofe que el a침o pasado cost칩 la vida a numerosas personas en el t칰nel del tauern y en el que despu칠s hubo que reconstruir durante largos meses y con un gigantesco gasto financiero lo que fue destruido por el incendio en el t칰nel \\</s>\n",
    "\n",
    "y al dividirlo en tokens se obtiene:\n",
    "\n",
    "['\\<s>',\n",
    " 'yo',\n",
    " 'como',\n",
    " 'austr칤aco',\n",
    " 'pero',\n",
    " 'creo',\n",
    " 'que',\n",
    " 'todos',\n",
    " 'nosotros',\n",
    " 'tenemos',\n",
    " 'a칰n',\n",
    " 'un',\n",
    " 'vivo',\n",
    " 'recuerdo',\n",
    " 'de',\n",
    " 'la',\n",
    " 'cat치strofe',\n",
    " 'que',\n",
    " 'el',\n",
    " 'a침o',\n",
    " 'pasado',\n",
    " 'cost칩',\n",
    " 'la',\n",
    " 'vida',\n",
    " 'a',\n",
    " 'numerosas',\n",
    " 'personas',\n",
    " 'en',\n",
    " 'el',\n",
    " 't칰nel',\n",
    " 'del',\n",
    " 'tauern',\n",
    " 'y',\n",
    " 'en',\n",
    " 'el',\n",
    " 'que',\n",
    " 'despu칠s',\n",
    " 'hubo',\n",
    " 'que',\n",
    " 'reconstruir',\n",
    " 'durante',\n",
    " 'largos',\n",
    " 'meses',\n",
    " 'y',\n",
    " 'con',\n",
    " 'un',\n",
    " 'gigantesco',\n",
    " 'gasto',\n",
    " 'financiero',\n",
    " 'lo',\n",
    " 'que',\n",
    " 'fue',\n",
    " 'destruido',\n",
    " 'por',\n",
    " 'el',\n",
    " 'incendio',\n",
    " 'en',\n",
    " 'el',\n",
    " 't칰nel',\n",
    " '\\</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e3aa2",
   "metadata": {},
   "source": [
    "### Conteo de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72996d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dict(text_array):\n",
    "    my_dictionary = {}\n",
    "    for word in text_array:\n",
    "        for item in word:\n",
    "            if item in my_dictionary:\n",
    "                my_dictionary[item] = my_dictionary[item] + 1\n",
    "            else:\n",
    "                my_dictionary[item] = 1\n",
    "    return my_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fdfe2",
   "metadata": {},
   "source": [
    "La funci칩n fill_dict crea un diccionario a partir de un arreglo de strings usando cada palabra como\n",
    "llave. Si la llave ya est치, se aumenta su cuenta, sino se le asigna un 1. Esta funci칩n se usa para\n",
    "contar la frecuencia de los tokens, tanto de unigramas como de bigramas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a123c",
   "metadata": {},
   "source": [
    "### Creaci칩n de matriz de frecuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e09450",
   "metadata": {},
   "source": [
    "Una vez que se obtuvieron los diccionarios de unigramas y bigramas creamos una matriz cuadrada de tama침o del vocabulario y 2 arreglos usando las llaves de los diccionarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2aafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.zeros(shape=(len(dataset_dict_unigrams) , len(dataset_dict_unigrams) ))\n",
    "rows = list(dataset_dict_unigrams.keys())\n",
    "cols = list(dataset_dict_unigrams.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5f6ae",
   "metadata": {},
   "source": [
    "Posteriormente llenamos la matriz con la ocurrencia de los bigramas utilizando la fila y la columna correspondiente, de acuerdo a los datos en el diccionario de bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(rows)):\n",
    "    for y in range(0, len(cols)):\n",
    "        mat[x,y] = search(rows[x], cols[y], dataset_dict_bigrams)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93c594fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(x,y, bigram_dict):\n",
    "    dict_key = (x,y)\n",
    "    return bigram_dict.get(dict_key,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff6172",
   "metadata": {},
   "source": [
    "La funci칩n search nos permite buscar un bigrama en el diccionario de bigramas y obtener su frecuencia. En caso de que no se encuentre, devuelve un 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce16f0",
   "metadata": {},
   "source": [
    "Finalmente, creamos un dataframe con que tendr치 como columnas y filas las llaves de los diccionarios y cuyos datos estar치n representados por la matriz de frecuencias mostrada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mat,\n",
    "    index=list(dataset_dict_unigrams.keys()),\n",
    "    columns=list(dataset_dict_unigrams.keys())\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc907b0",
   "metadata": {},
   "source": [
    "### Estimador de M치xima Verosimilitud (Maximum Likelihood Estimation, MLE) y suavizado de Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6af553",
   "metadata": {},
   "source": [
    "Para calcular la probabilidad de una palabra 洧녽 dada una palabra previa $x$,\n",
    "se calcula el conteo de los bigramas $C(xy)$ y se normaliza con todos los\n",
    "bigramas que comparten la primera palabra $x$, que es lo mismo que los\n",
    "unigramas de $x$, lo que se obtiene con la siguiente f칩rmula:\n",
    "\n",
    "$P\\left ( y|x \\right ) = \\frac{C\\left ( xy \\right )}{C\\left ( x \\right )}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78e2cd",
   "metadata": {},
   "source": [
    "Por otra parte, el suavizado de Laplace o ley de Laplace es una t칠cnica sencilla que consiste en proporcionar un poco del espacio de probabilidades a los eventos no vistos.\n",
    "Para aplicar suavizado de Laplace a bigramas  se requiere aumentar el conteo de unigramas con el total de tipos de palabras en el vocabulario V. \n",
    "\n",
    "Esto es: $P_{Lap} \\left ( w_{2} | w_{1} \\right ) = \\frac{C\\left ( w_{1} w_{2} \\right ) + 1}{C\\left ( w_{1} \\right ) + V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4598236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_x_y_df(xy, df, unigram_dict, laplace = False):\n",
    "    if(laplace):\n",
    "        return ( df.loc[xy[0]][xy[1]] + 1) /  ( unigram_dict[xy[0]] + len(unigram_dict))\n",
    "    return  df.loc[xy[0]][xy[1]]  /  unigram_dict[xy[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0ff0f",
   "metadata": {},
   "source": [
    "La funci칩n anterior se encarga del c치lculo de MLE. El primer par치metro es un bigrama, el segundo un dataframe que contiene las frecuencias y el tercero un diccionario que contiene el vocabulario. Se incluye un par치metro llamado laplace (predeterminado falso) para utilizar suavizado de Laplace en caso de ser necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610304e",
   "metadata": {},
   "source": [
    "### Probabilidad de una secuencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0196cd0",
   "metadata": {},
   "source": [
    "Para calcular la probabilidad de una secuencia se usa la siguiente f칩rmula: \n",
    "$P_{MLE}\\left ( w_{1} ... w_{n} \\right ) \\approx \\prod_{k=1}^{n} P\\left ( w_{k} | w_{k-1} \\right )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af9a29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_probability(text, dataf, unigram_dict, laplace = False):\n",
    "    result = 1\n",
    "    processed_text = preprocess(text)\n",
    "    bigrams = list(ngrams(processed_text, 2))\n",
    "    for i in bigrams:\n",
    "        result = result*p_x_y_df(i, df, unigram_dict, laplace)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e0c56",
   "metadata": {},
   "source": [
    "Mediante la funci칩n anterior se puede calcular la probabilidad de una secuencia. Recibe como par치metros la secuencia, el dataframe de las frecuencias,\n",
    ", el diccionario que contiene el vocabulario y el par치metro opcional laplace que nos permite utilizar suavizado de Laplace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37223d92",
   "metadata": {},
   "source": [
    "### Obtener las siguientes n palabras m치s probables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a047e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_likely(word, top_n, laplace = False):\n",
    "    los_ser = df.loc[word]\n",
    "    results = {}\n",
    "    for indx in los_ser.items():\n",
    "        xy = (word, indx[0])    \n",
    "        results[xy] = p_x_y_df(xy, df, dataset_dict_unigrams, laplace)\n",
    "    ordered = sorted(results.items(), key=lambda x:x[1], reverse=True)\n",
    "    return list(map(lambda x: x[0][1], ordered[:top_n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2bc1b8",
   "metadata": {},
   "source": [
    "Utilizando la funci칩n anteriormente descrita podemos obtener las siguientes n palabras m치s probables. El primer par치metro es el n칰mero de palabras deseadas y el segundo es opcional e indica si queremos usar suavizado de Laplace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072a0f9",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a94619",
   "metadata": {},
   "source": [
    "### Obteniendo la probabilidad de una oraci칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b5e76",
   "metadata": {},
   "source": [
    "Usando los modelos de probabilidad MLE y MLE con suavizado de Laplace, se calcular치n las probabilidades de las siguientes oraciones:\n",
    "\n",
    "* el parlamento debe enviar un mensaje\n",
    "* el parlamento debe enviar un consejo\n",
    "* el abismo entre pobres y ricos\n",
    "* el abismo entre ricos y pobres\n",
    "* el abismo de la cantera entre pobres y ricos\n",
    "* la comisi칩n debe ser totalmente transparente\n",
    "* la comisi칩n debe ser transparente\n",
    "\n",
    "Los resultados obtenidos se pueden observar en la siguiente tabla:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fede8",
   "metadata": {},
   "source": [
    "| Oraci칩n      | Probabilidad MLE | Probabilidad MLE con suavizado de Laplace |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| el parlamento debe enviar un mensaje      | 4.4374617510995097e-13       | 5.988775375404499e-21       |\n",
    "| el parlamento debe enviar un consejo   | 3.3572778663495715e-13        | 9.358930610679253e-20       |\n",
    "| el abismo entre pobres y ricos      | 3.807892655558683e-17       | 1.6730189594858084e-26       |\n",
    "| el abismo entre ricos y pobres      | 8.648730905039033e-15       | 1.16115482545265e-24       |\n",
    "| el abismo de la cantera entre pobres y ricos      | 0       | 2.2100768814795686e-37       |\n",
    "| la comisi칩n debe ser totalmente transparente      | 3.59796250589801e-11       | 7.49629030518359e-19       |\n",
    "| la comisi칩n debe ser transparente      | 2.5521547375169884e-09       | 4.12560040648121e-15       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414108bb",
   "metadata": {},
   "source": [
    "Podemos observar que las probabilidades obtenidas usando MLE son mayores que las que fueron calculadas usando suavizado de Laplace.\n",
    "Es importante notar que en el caso de \"el abismo de la cantera entre pobres y ricos\", la probabilidad usando MLE es 0 debido a que el bigrama ('la', 'cantera') \n",
    "no ocurre. Sin embargo, al usar MLE con suavizado de Laplace podemos evitar este problema ya que los bigramas que no ocurrieron en el conjunto de datos al menos tendr치n una ocurrencia. Esto nos permite obtener una probabilidad para la oraci칩n listada anteriormente. No obstante, es poco probable que ocurra dicha oraci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2c6bf",
   "metadata": {},
   "source": [
    "### Predicci칩n de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737dd1f",
   "metadata": {},
   "source": [
    "Usando un modelo de n-gramas tambi칠n se pueden realizar predicci칩n de palabras, esto es ,dada una palabra inicial mostrar las siguientes palabras m치s probables de acuerdo con los modelos MLE y MLE con suavizado de Laplace.\n",
    "A continuaci칩n se muestran las cinco palabras m치s probables para cada palabra de una oraci칩n dada (las de mayor probabilidad aparecen primero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f73d8d",
   "metadata": {},
   "source": [
    "Para la secuencia *los tribunales nacionales* obtenemos lo siguiente:\n",
    "\n",
    "los ['estados', 'pa칤ses', 'derechos', 'que', 'ciudadanos']\n",
    "\n",
    "tribunales ['nacionales', 'de', '</s>', 'y', 'en']\n",
    "\n",
    "nacionales ['</s>', 'de', 'y', 'en', 'que']\n",
    "\n",
    "Podemos observar que para la palabra *los*, las cinco palabras siguientes m치s probables son *estados*, *paises*, *derechos*, *que* y *ciudadanos*; luego para la palabra *tribunales* las cinco palabras siguientes m치s probables son *nacionales*, *de*, *\\</s>*, *y*, y *en* y finalmente para la palabra *nacionales*, *\\</s>*, *de*,*y*,*en* y *que*. Es importante resaltar que el modelo construido nos permite predecir que la siguiente palabra m치s probable despu칠s de *tribunales* ser치 *nacionales* para formar la secuencia *tribunales nacionales*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672c319",
   "metadata": {},
   "source": [
    "Para la palabra *la*, obtenemos lo siguiente:\n",
    "    \n",
    "la ['comisi칩n', 'uni칩n', 'pol칤tica', 'sra', 'ue']\n",
    "\n",
    "Observamos que las cinco palabras siguientes m치s probables son *comisi칩n*, *uni칩n*, *pol칤tica*, *sra* y *ue*.\n",
    "\n",
    "Para el caso de la palabra *parlamento*, tenemos que:\n",
    "\n",
    "parlamento ['europeo', '</s>', 'y', 'en', 'que']\n",
    "\n",
    "N칩tese que las cinco palabras siguientes m치s probables son *europeo*, *\\</s>*, *y*, *en* y *que*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25357662",
   "metadata": {},
   "source": [
    "A continuaci칩n se realizan 3 ejemplos de predicci칩n utilizando el modelo que se construy칩:\n",
    "* Para la secuencia *el en consejo del parlamento* se obtiene: \n",
    "\n",
    "        en ['el', 'la', 'los', 'este', 'las']\n",
    "        \n",
    "        el ['parlamento', 'consejo', 'sr', 'informe', 'que']\n",
    "        \n",
    "        consejo ['de', 'y', '</s>', 'europeo', 'que']\n",
    "        \n",
    "        del ['consejo', 'parlamento', 'sr', 'grupo', 'd칤a']\n",
    "        \n",
    "        parlamento ['europeo', '</s>', 'y', 'en', 'que']\n",
    "        \n",
    "* Para la secuencia *en muchos paises* se obtiene:\n",
    "\n",
    "        en ['el', 'la', 'los', 'este', 'las']\n",
    "        \n",
    "        muchos ['de', 'a침os', 'casos', 'pa칤ses', 'otros']\n",
    "        \n",
    "        pa칤ses ['de', 'en', 'que', 'candidatos', '</s>']\n",
    "        \n",
    "* Para la secuencia *gasto p칰blico* se tiene:\n",
    "\n",
    "        gasto ['de', 'en', 'p칰blico', 'agr칤cola', '</s>']\n",
    "        \n",
    "        p칰blico ['</s>', 'y', 'en', 'de', 'a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b2d23",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "Los bigramas son muy 칰tiles para la predicci칩n de palabras, ya sea el c치lculo de probabilidad de una secuencia o predecir la siguiente palabra en una oraci칩n. Sin embargo, son muy dependientes del conjunto de entrenamiento que usemos y son sensibles a encontrarse con probabilidades cero de n-gramas, por lo que es necesario implementar t칠cnicas como el suavizado de Laplace, Backoff, descuento de Good-Turing, etc.,  para atenuar este problema. A pesar de que en la actualidad existen modelos m치s complejos como Redes Neuronales Recurrentes o Modelo de Lenguaje Colosal, los n-gramas proporcionan un buen enfoque para tareas de PLN y son f치ciles de implementar. Asimismo, son fundamentales para entender las tareas de modelado de lenguaje y pueden ser usados en gran variedad de aplicaciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
